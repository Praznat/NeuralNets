




Neural Factored-State Transition Modeler

Goals:
1. Use neural networks to learn an approximate encoding of an environment (Markov decision process)
2. Avoid state space explosion by keeping network complexity only polynomial in the number of state variables
3. Achieve a generative model such that, given a current state and action, it can guess the subsequent state
with some measure of performance (probably approximately correct or statistically significantly better than random)
4. Learned model should be independent from planner and reward function

We will call a model learner that achieves the above goals a Neural Factored-State Transition Modeler (NFSTM)

Achievements	Policy-based	Value-function	Approx.Value-function	Bayes Net	NFSTM
1				N				Y				Y						N			Y
2				Y				N				Y						?			Y
3				N				N				N						Y			Y
4				N				N				N						Y			Y

Definitions:
1. Transition
A Markov decision process is defined by transitions, which are represented as probabilities of moving to
a new state given a current state and action, such that all such probabilities from a current state and
action sum to 1.
2. Pre-state variables X
The values of the variables that comprise the current state (the "before" state in a transition)
3. Action variables A
The values of the variables that comprise an action that affects the transition
4. Post-state variables X'
The values of the variables that comprise the subsequent state (the "after" state in a transition)

Components:
1. Variable Transition Predictor a.k.a. "predictor"
This is a neural network that learns to associate X and A with X'. It learns at each step by feeding
the current state variables and action variables into the inputs, then transitioning to a next state
and using its variables as the outputs, then employing some learning method such as backpropagation.
Over time, it will learn to predict changes in the state variables caused by the transitions. In order
to generate a guess for a subsequent state given a current state and action, it feeds forward the 
current state and action variables to obtain outputs ranging from 0 to 1. The predicted state is composed
of variables of value 0 or 1 given by probabilities equaling the network outputs. If all of the state
variables are discrete and independent of one another, the VTA will be sufficient to model transitions.
DBNs : "factored states"
2. Variable Relationship Modeler a.k.a. "conditioner"
If some variables are correlated, another step is necessary to ensure only realistic states are
predicted. To illustrate, consider the following two example environments.
In environment 1, the only two reachable states from x are [0,0] and [1,1].
In environment 2, the only two reachable states from x are [1,0] and [0,1].
Suppose in both environments the probability of reaching each state from x is 50%. Now in both cases, a neural network model learner will learn to output
approximately {0.5, 0.5} given input x, and all four states will be sampled at equal rates for
both environments. In order to only sample states that are reachable in the learned environment, the
predicted state from the VTA is fed into an IRM neural network to arrive at more appropriate outputs.
This step can be repeated multiple times for better results. The IRM is trained simultaneously with the VTA.
At each transition, the inputs to the IRM are all variables X, A, and X', and the target output is X'.
Thus, the IRM approximates X' given an initial guess of X' using learned correlations between variables.


Qualities of NFSTM
1. Supervised learning that uses backpropagation (ProbabilityTracking, BasicTests +-CORR)
2. Integrates with logical planner (DecisionProcess)
3. Relatively low complexity (Comparison with tables)
4. Learns stochastic environments (Geysers)
5. Separates reward module (GridTagGame, Collector, axistransfer)
	a. Object modularization (GridTagGame?, axistransfer)
	b. examples: axistransfer collect x, avoid x, get right in front of x then dodge, etc
6. Transfer learning (AxisTransfer)
7. Abduction (Backward vs forward reasoning)
8. Biology (That paper)
9. Learning instincts (Model -> policy)

neural net based value function approximation: has to retrain every time the reward function changes

caveats: when to stop training, how good is "good" error, when are there not enough training samples,
when the neural net architecture is appropriate or not, etc

"chainprocessing":
Experience/observations -> model learner -> planner -> actions
beliefs vs desires

weight-decay and symbolization
redundancy -> transfer
find several similar subnetworks throughout net that can be replaced by rerouting through single subnetwork
symbolismo
transferability measure : bisimulation metric

domains		truth-teller / liar games
			maze (always centered on agent; goal is just move down)
			asterisk (atari)

Why Model-based RL
Q-learning not PAC-MDP, model-based can be PAC-MDP (Littman)
Separate reward logic, modularity

Why Neural Networks?
Even without traversing the entire state space, a neural network may capture patterns that allow an agent
to make "educated guesses" about parts of the state space it has not trained on. This ability has been exploited
already by reinforcement learning methods based on "approximate value functions". The difference being that
instead of only trying to approximate the reward based on state variables, the whole model learner approximates
the transitions in all of the state variables.
Deep learning
Modularization (cite pruninggood) -> objectification (cite littman)

Why separate reward function?
Learning is its own reward; play behavior in animals; learning about environment before reward is relevant;
adapting to new and changing reward functions
Learning reward function is separate problem
"goal-based" (cite neuro stuff) decision of goal can be volatile (just because i forget what im doing doesnt mean
i forget how the world works)

Why separate planning?
Modularity

Why not policy based?
Faster (supervised learning)


neuro-symbolic
1. connectionist model-learner + symbolic planning
2. neural net abduction
3. symbolic environment description and comparison


interesting directions

POMDP might not know what state you are in but estimate from past states and actions!

Explicitly separating independent rules (player movement vs enemy movement)
modularization -> discretization
"Objects with attributes rather than states with features" (Littman)
a.k.a. automatic object-orientation

learning specialist: feed chosen optimal actions to output target given state as input (policy-based)

About learning hidden state variables

cellular automata -> HyperNEAT can reuse patterns!

Environments with Continuous Variables?

instinct	-	habit	-	reason
policy		-	value	-	model

novelty search should reduce exploration time
throwing out similar samples during training should speed up training

cite other neuro-sybolic papers
cite approx dynamic programming

https://www.cis.upenn.edu/~mkearns/papers/reinforcement.pdf
polynomial in number of STATES