




Neural Factored-State Transition Modeler

Goals:
1. Use neural networks to learn an approximate encoding of an environment (Markov decision process)
2. Avoid state space explosion by keeping network complexity only polynomial in the number of state variables
3. Achieve a generative model such that, given a current state and action, it can guess the subsequent state
with some measure of performance (probably approximately correct or statistically significantly better than random)
4. Learned model should be independent from planner and reward function

We will call a model learner that achieves the above goals a Neural Factored-State Transition Modeler (NFSTM)

Achievements	Policy-based	Value-function	Approx.Value-function	Bayes Net	NFSTM
1				N				Y				Y						N			Y
2				Y				N				Y						?			Y
3				N				N				N						Y			Y
4				N				N				N						Y			Y

Definitions:
1. Transition
A Markov decision process is defined by transitions, which are represented as probabilities of moving to
a new state given a current state and action, such that all such probabilities from a current state and
action sum to 1.
2. Pre-state variables X
The values of the variables that comprise the current state (the "before" state in a transition)
3. Action variables A
The values of the variables that comprise an action that affects the transition
4. Post-state variables X'
The values of the variables that comprise the subsequent state (the "after" state in a transition)

Components:
1. Variable Transition Predictor a.k.a. "predictor"
This is a neural network that learns to associate X and A with X'. It learns at each step by feeding
the current state variables and action variables into the inputs, then transitioning to a next state
and using its variables as the outputs, then employing some learning method such as backpropagation.
Over time, it will learn to predict changes in the state variables caused by the transitions. In order
to generate a guess for a subsequent state given a current state and action, it feeds forward the 
current state and action variables to obtain outputs ranging from 0 to 1. The predicted state is composed
of variables of value 0 or 1 given by probabilities equaling the network outputs. If all of the state
variables are discrete and independent of one another, the VTA will be sufficient to model transitions.
DBNs : "factored states"
2. Variable Relationship Modeler a.k.a. "conditioner"
If some variables are correlated, another step is necessary to ensure only realistic states are
predicted. To illustrate, consider the following two example environments.
In environment 1, the only two reachable states from x are [0,0] and [1,1].
In environment 2, the only two reachable states from x are [1,0] and [0,1].
Suppose in both environments the probability of reaching each state from x is 50%. Now in both cases, a neural network model learner will learn to output
approximately {0.5, 0.5} given input x, and all four states will be sampled at equal rates for
both environments. In order to only sample states that are reachable in the learned environment, the
predicted state from the VTA is fed into an IRM neural network to arrive at more appropriate outputs.
This step can be repeated multiple times for better results. The IRM is trained simultaneously with the VTA.
At each transition, the inputs to the IRM are all variables X, A, and X', and the target output is X'.
Thus, the IRM approximates X' given an initial guess of X' using learned correlations between variables.


Qualities of NFSTM
1. Supervised learning that uses backpropagation (ProbabilityTracking, BasicTests +-CORR)
2. Integrates with logical planner (DecisionProcess)
3. Relatively low complexity (Comparison with tables)
4. Learns stochastic environments (Geysers)
5. Separates reward module (GridTagGame, Collector, axistransfer)
	a. Object modularization (GridTagGame?, axistransfer)
	b. examples: axistransfer collect x, avoid x, get right in front of x then dodge, etc
6. Transfer learning (AxisTransfer)
7. Abduction (Backward vs forward reasoning)
8. Biology (That paper)
9. Learning instincts (Model -> policy)

neural net based value function approximation: has to retrain every time the reward function changes

caveats: when to stop training, how good is "good" error, when are there not enough training samples,
when the neural net architecture is appropriate or not, etc

"chainprocessing":
Experience/observations -> model learner -> planner -> actions
beliefs vs desires

weight-decay and symbolization
redundancy -> transfer
find several similar subnetworks throughout net that can be replaced by rerouting through single subnetwork
symbolismo
transferability measure : bisimulation metric

domains		truth-teller / liar games
			maze (always centered on agent; goal is just move down)
			asterisk (atari)

Why Model-based RL
Q-learning not PAC-MDP, model-based can be PAC-MDP (Littman)
Separate reward logic, modularity

Why Neural Networks?
Even without traversing the entire state space, a neural network may capture patterns that allow an agent
to make "educated guesses" about parts of the state space it has not trained on. This ability has been exploited
already by reinforcement learning methods based on "approximate value functions". The difference being that
instead of only trying to approximate the reward based on state variables, the whole model learner approximates
the transitions in all of the state variables.
Deep learning
Modularization (cite pruninggood) -> objectification (cite littman)

Why separate reward function?
Learning is its own reward; play behavior in animals; learning about environment before reward is relevant;
adapting to new and changing reward functions
Learning reward function is separate problem
"goal-based" (cite neuro stuff) decision of goal can be volatile (just because i forget what im doing doesnt mean
i forget how the world works)

Why separate planning?
Modularity

Why not policy based?
Faster (supervised learning)


neuro-symbolic
1. connectionist model-learner + symbolic planning
2. neural net abduction
3. symbolic environment description and comparison


interesting directions

POMDP might not know what state you are in but estimate from past states and actions!

Explicitly separating independent rules (player movement vs enemy movement)
modularization -> discretization
"Objects with attributes rather than states with features" (Littman)
a.k.a. automatic object-orientation

learning specialist: feed chosen optimal actions to output target given state as input (policy-based)

About learning hidden state variables

cellular automata -> HyperNEAT can reuse patterns!

Environments with Continuous Variables?

instinct	-	habit	-	reason
policy		-	value	-	model

novelty search should reduce exploration time
throwing out similar samples during training should speed up training

cite other neuro-sybolic papers
cite approx dynamic programming

https://www.cis.upenn.edu/~mkearns/papers/reinforcement.pdf
polynomial in number of STATES








TRANSFER

what you already have:
avoid vs catch (transfer of model in light of new reward)

next steps:
how to transfer - show that transferring OWN network for SAME game is
faster than learning from scratch
	what does faster mean? sample efficiency
	experiment: using sandwiched network after only few samples (vs control)
when to transfer - show some method for source selection
	idea 1: bisimulation
	idea 2: sandwich several and hope it connects to the best
if you can guess what game this is from small number of samples, you can plug in the correct network
jumpstart for Fliers2FlyCells from simple plug-in (not sandwich, just same network)?
	
translation networks
problem of variable mapping
learn and store networks that map between variables
use most common ones most often
for example:
	1-1 mapping
	axis rotation
	axis flip
	scaling
choose one that best fits new (few) samples
if no fit, then learn from scratch
learn input-to-input mapping network, output-to-output mapping network

Sample efficiency!
axis transfer: youve learned the horizontal game.
now you play the vertical version but you only see a couple samples.
can you reuse the horizontal modeler to help you play the vertical one
even only after having seen a couple samples of the vertical one?

1. train horizontal player
2. show a few examples D of vertical game
3. train control C from scratch (or random) on D
4. train transfer learner T reusing frozen horizontal modeler on D
5. compare C vs T

if vertical vs horizontal doesnt work (problem of variable translation):
fliers but no catcher vs catcher but no fliers


Generalization, modularization, objectification, prototypization, etc.

Have limited number of Network Prototypes
Network prototype specifies conditional relationship network between an output variable
and a set of input variables specified by the Relationship Function with the output variable

learn relationships from experience
cell A depends on cell B, cell C depends on cell D,
A and C are same type, B and D are same type, both relationships are of same type

HyperModeler: model the models
Network connecting nodes depends on the properties of the variables represented by those
nodes and on the relationships between those variables
Example: Fliers
if (outclass == 1 && inclass == 1 && outcol < rightmost
	&& inrow == outrow && incol = outcol + 1)
	-> This is a positive conditional network
	
Transfer: change in CPPN rules BUT NOT variable properties, variable relations, or output options
Example: rotate fliers
if (outclass == 1 && inclass == 1 && outrow < bottommost
	&& incol == outcol && inrow = outrow + 1)
	-> This is a positive conditional network
	
QUESTION: for transfer, how to change the CPPN so that the final model best matches the
new (few) samples in the target task